# LoRA Fine-tuning Configuration
# Based on recommendations for LLaMA-7B on 50K instructions

# Model and Data
model: "meta-llama/Meta-Llama-3.1-8B-Reference"
train_file: "/workspace/distilled-alignment/distilled-alignment/data/alpaca/alpaca_train.jsonl"
val_file: "/workspace/distilled-alignment/distilled-alignment/data/alpaca/alpaca_val.jsonl"

# Training Parameters
n_epochs: 2  # Recommended 3-5 epochs for LoRA
batch_size: 16  # Effective batch size ~128
learning_rate: 2e-4  # Higher LR for LoRA (vs 2e-5 for full fine-tuning)
n_evals: 5
n_checkpoints: 2

# LoRA Configuration
lora: true
lora_r: 16  # LoRA rank
lora_alpha: 16  # LoRA alpha (typically equals lora_r)
lora_dropout: 0.05  # LoRA dropout

# Model Suffix and Naming
suffix: "llama-8b-all-lr-2e-4"

# Logging and Monitoring
wandb_project_name: "llama-8b-all-lr-2e-4"
wandb_entity: "emil-experiments"  # Set to your wandb entity if needed
tags: ["lora", "llama-8b", "alpaca"]

# Output Configuration
save_folder: "output_together_finetunes/"
save_model: false

# Other Settings
logging_level: "info"
dry_run: false